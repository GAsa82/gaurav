import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('data/your_dataset.csv')

# Dataset Info
print("\n--- Dataset Info ---")
print(df.info())

# Summary Stats
print("\n--- Summary Statistics ---")
print(df.describe(include='all'))

# Missing Values
print("\n--- Missing Value Count ---")
missing_values = df.isnull().sum()
print(missing_values)

missing_percentage = (missing_values / len(df)) * 100
missing_report = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})
print(missing_report)

# Duplicate Rows
duplicates = df.duplicated()
print(f"Duplicate rows found: {duplicates.sum()}")

# Data Type Consistency
print("\n--- Data Types ---")
print(df.dtypes)

# Outlier Detection
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
    print(f"{col}: {len(outliers)} outliers")

# Data Quality Scorecard
def quality_scorecard(df):
    total_rows = len(df)
    missing_rate = df.isnull().sum().sum() / df.size * 100
    duplicate_rate = df.duplicated().sum() / total_rows * 100
    quality_score = 100 - (missing_rate + duplicate_rate)
    print("\n--- Data Quality Scorecard ---")
    print(f"Missing Rate: {missing_rate:.2f}%")
    print(f"Duplicate Rate: {duplicate_rate:.2f}%")
    print(f"Data Quality Score: {quality_score:.2f}/100")

quality_scorecard(df)
